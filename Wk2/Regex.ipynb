{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd590e00",
   "metadata": {},
   "source": [
    "## Practical 2 – Traditional Linguistic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U spacy nltk jieba transformers\n",
    "import sys\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f820650",
   "metadata": {},
   "source": [
    "### Level 1 – Core Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "base_text = (\n",
    "    \"Email: jane.doe@uni.edu NLP AI 机器学习 <nlp@northampton.ac.uk> \"\n",
    "    \"Message us at pro-team@dept.co.uk and tag #NLP #RegEx.\"\n",
    ")\n",
    "updated_text = (\n",
    "    base_text\n",
    "    + \" Connect via outreach@cs.school.ac.uk and extra@multi.research.lab.uk\"\n",
    "    + \" 加入话题 #人工智能 #机器学习\"\n",
    ")\n",
    "\n",
    "pattern_email = r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\"\n",
    "pattern_hashtag = r\"(?<!\\w)#[\\w]+\"\n",
    "\n",
    "emails_base = re.findall(pattern_email, base_text)\n",
    "hashtags_base = re.findall(pattern_hashtag, base_text)\n",
    "emails_updated = re.findall(pattern_email, updated_text)\n",
    "hashtags_updated = re.findall(pattern_hashtag, updated_text)\n",
    "\n",
    "print(\"Base emails:\", emails_base)\n",
    "print(\"Base hashtags:\", hashtags_base)\n",
    "print(\"Updated emails:\", emails_updated)\n",
    "print(\"Updated hashtags:\", hashtags_updated)\n",
    "print(\"Counts → base emails:\", len(emails_base), \"base hashtags:\", len(hashtags_base))\n",
    "print(\"Counts → updated emails:\", len(emails_updated), \"updated hashtags:\", len(hashtags_updated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4628f9",
   "metadata": {},
   "source": [
    "*Emails remain detectable with multi-dot domains; the Unicode-friendly hashtag regex still captures Chinese tags, while ASCII-only classes would miss them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download('en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = [\n",
    "    \"Dr. Olu's AI-based model outperforms others in 2025.\",\n",
    "    \"He said, \"Let's deploy version 2.0--no delays this time,\" before leaving at 5:45 p.m.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    spacy_tokens = [token.text for token in nlp(sentence)]\n",
    "    nltk_tokens = word_tokenize(sentence)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"spaCy :\", spacy_tokens)\n",
    "    print(\"NLTK  :\", nltk_tokens)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914c360",
   "metadata": {},
   "source": [
    "*spaCy keeps hyphenated compounds and contractions tighter than the default NLTK splitter, which over-fragments punctuation-heavy phrases.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb25c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "base_words = [\"studies\", \"studying\", \"better\", \"ate\", \"flies\"]\n",
    "\n",
    "for word in base_words:\n",
    "    print(word, \"→\", porter.stem(word), \"→\", lemmatizer.lemmatize(word, \"v\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbe3aa",
   "metadata": {},
   "source": [
    "*Lemmatisation preserves verbal semantics (`ate` → `eat`), whereas the stemmer collapses forms aggressively—useful for recall-heavy IR despite semantic loss.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jieba\n",
    "\n",
    "text_cn = \"我喜欢学习人工智能\"\n",
    "segments = list(jieba.cut(text_cn))\n",
    "print(segments)\n",
    "assert ''.join(segments) == text_cn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0f9f8",
   "metadata": {},
   "source": [
    "*Chinese lacks whitespace token boundaries, so segmentation is mandatory before models expecting token streams can operate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12003868",
   "metadata": {},
   "source": [
    "### Level 2 – Extended Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def extract_handles_loose(text):\n",
    "    pattern = r\"(?:(?<=\\s)|^)@[A-Za-z0-9_]{1,15}\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def extract_handles(text):\n",
    "    pattern = r\"(?<!\\w)@[A-Za-z0-9_]{1,15}\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "sample = (\n",
    "    \"Follow @nlp_lab and @Research_AI. Email team@uni.edu or admin@example.com.\"\n",
    ")\n",
    "punct_sample = \"Nice work.@edge_user! Ping @ok_team next.\"\n",
    "\n",
    "print(\"Loose sample:\", extract_handles_loose(sample))\n",
    "print(\"Improved sample:\", extract_handles(sample))\n",
    "print(\"Loose punctuation:\", extract_handles_loose(punct_sample))\n",
    "print(\"Improved punctuation:\", extract_handles(punct_sample))\n",
    "assert extract_handles(sample) == ['@nlp_lab', '@Research_AI']\n",
    "assert extract_handles(punct_sample) == ['@edge_user', '@ok_team']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e9a7a",
   "metadata": {},
   "source": [
    "*Negative lookbehind blocks email matches while still accepting punctuation-adjacent handles; accepting `.` via a lookbehind tweak catches `.@user` cases.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    nlp\n",
    "except NameError:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    pattern = r\"(?:[A-Za-z]+(?:-[A-Za-z]+)+)|[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\w\\s]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "text = \"Dr. A. I. Jones co-authored a study, didn't he?\"\n",
    "print(\"Simple:\", simple_tokenize(text))\n",
    "print(\"spaCy :\", [token.text for token in nlp(text)])\n",
    "assert \"co-authored\" in simple_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247cc8d",
   "metadata": {},
   "source": [
    "*The augmented regex keeps hyphenated compounds intact; spaCy still provides richer linguistic features, but the custom rule curbs token inflation for analytical scripts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_words = [\"studies\", \"studying\", \"better\", \"ate\", \"flies\", \"running\", \"mice\"]\n",
    "for word in extended_words:\n",
    "    print(word, \"→\", porter.stem(word), \"→\", lemmatizer.lemmatize(word, \"v\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a0e2b",
   "metadata": {},
   "source": [
    "*Lemma outputs (`running` → `run`, `mice` → `mouse`) stay interpretable; stems flatten irregular plurals, which might be tolerable in lightweight retrieval systems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEX = {\"我\", \"喜欢\", \"学习\", \"人工智能\", \"自然语言\", \"处理\"}\n",
    "\n",
    "def max_match(text, lexicon):\n",
    "    output, index = [], 0\n",
    "    while index < len(text):\n",
    "        for end in range(len(text), index, -1):\n",
    "            fragment = text[index:end]\n",
    "            if fragment in lexicon:\n",
    "                output.append(fragment)\n",
    "                index = end\n",
    "                break\n",
    "        else:\n",
    "            output.append(text[index])\n",
    "            index += 1\n",
    "    return output\n",
    "\n",
    "sentence = \"我喜欢学习人工智能\"\n",
    "print(\"Greedy:\", max_match(sentence, LEX))\n",
    "print(\"Jieba :\", list(jieba.cut(sentence)))\n",
    "LEX.update({\"自然语言处理\"})\n",
    "print(\"Greedy with extended lexicon:\", max_match(sentence, LEX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715501",
   "metadata": {},
   "source": [
    "*Greedy longest-match falls back to single characters without dictionary coverage; enriching the lexicon narrows the gap but remains brittle versus statistical segmenters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5fd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "english = \"I love natural language processing.\"\n",
    "chinese = \"我喜欢自然语言处理。\"\n",
    "print(\"EN tokens:\", len(tokenizer.tokenize(english)))\n",
    "print(\"ZH tokens:\", len(tokenizer.tokenize(chinese)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bea79",
   "metadata": {},
   "source": [
    "*Subword tokenisers inflate non-Latin scripts, so pay-per-token pricing can disadvantage certain languages—fairness demands either normalisation or pricing adjustments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3253fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'https://example.org'), ('Docs', 'https://docs.example.org')]\n",
      "['#09F', '#a1b2c3', '#123456']\n",
      "2024-03-18 True\n",
      "1999/12/31 True\n",
      "2100-01-01 False\n",
      "23:59 True\n",
      "07:30:15 True\n",
      "24:01 False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "markdown_pattern = re.compile(r\"\\[([^\\]]+)\\]\\((https?://[^\\s)]+)\\)\")\n",
    "hex_pattern = re.compile(r\"#(?:[0-9A-Fa-f]{3}|[0-9A-Fa-f]{6})\\b\")\n",
    "\n",
    "markdown_test = \"See [NLP](https://example.org) and [Docs](https://docs.example.org).\"\n",
    "hex_test = \"Palette: #09F, #a1b2c3, and #123456.\"\n",
    "\n",
    "print(markdown_pattern.findall(markdown_test))\n",
    "print(hex_pattern.findall(hex_test))\n",
    "\n",
    "date_pattern = re.compile(r\"\\b(19|20)\\d{2}[-/.](0[1-9]|1[0-2])[-/.](0[1-9]|[12]\\d|3[01])\\b\")\n",
    "time_pattern = re.compile(r\"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?::[0-5]\\d)?\\b\")\n",
    "\n",
    "dates = [\"2024-03-18\", \"1999/12/31\", \"2100-01-01\"]\n",
    "times = [\"23:59\", \"07:30:15\", \"24:01\"]\n",
    "\n",
    "for date in dates:\n",
    "    print(date, bool(date_pattern.fullmatch(date)))\n",
    "for time in times:\n",
    "    print(time, bool(time_pattern.fullmatch(time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2e224",
   "metadata": {},
   "source": [
    "*Markdown regex captures `[label](url)` syntax; the hex matcher supports short and full forms. The date regex restricts to 1900–2099, while the time regex covers 24-hour clocks with optional seconds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda326a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedbathandbeyond → ['bedbath', 'and', 'beyond']\n",
      "therapist → ['therapist']\n",
      "artificialintelligence → ['artificial', 'intelligence']\n",
      "Patched lexicon examples:\n",
      "bedbathandbeyond → ['bedbathandbeyond']\n",
      "therapist → ['therapist']\n",
      "artificialintelligence → ['artificial', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Set\n",
    "\n",
    "lexicon_en = {\n",
    "    \"bed\",\n",
    "    \"bath\",\n",
    "    \"bedbath\",\n",
    "    \"and\",\n",
    "    \"beyond\",\n",
    "    \"the\",\n",
    "    \"rapist\",\n",
    "    \"therapist\",\n",
    "    \"artificial\",\n",
    "    \"intelligence\",\n",
    "    \"art\",\n",
    "    \"ificial\",\n",
    "}\n",
    "\n",
    "def greedy_segment(text: str, lexicon: Set[str]) -> List[str]:\n",
    "    segments: List[str] = []\n",
    "    index = 0\n",
    "    while index < len(text):\n",
    "        for end in range(len(text), index, -1):\n",
    "            candidate = text[index:end]\n",
    "            if candidate in lexicon:\n",
    "                segments.append(candidate)\n",
    "                index = end\n",
    "                break\n",
    "        else:\n",
    "            segments.append(text[index])\n",
    "            index += 1\n",
    "    return segments\n",
    "\n",
    "examples = {\n",
    "    \"bedbathandbeyond\": lexicon_en,\n",
    "    \"therapist\": lexicon_en,\n",
    "    \"artificialintelligence\": lexicon_en,\n",
    "}\n",
    "\n",
    "for sample_text, dictionary in examples.items():\n",
    "    print(sample_text, \"→\", greedy_segment(sample_text, dictionary))\n",
    "\n",
    "patched_lexicon_en = set(lexicon_en)\n",
    "patched_lexicon_en.add(\"bedbathandbeyond\")\n",
    "\n",
    "print(\"Patched lexicon examples:\")\n",
    "for sample_text in examples:\n",
    "    print(sample_text, \"→\", greedy_segment(sample_text, patched_lexicon_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55e8b0",
   "metadata": {},
   "source": [
    "*English greedy segmentation behaves like the Chinese variant: it succeeds when multi-word entries exist (`bedbathandbeyond`) and misfires on ambiguous strings (`therapist` → `the`, `rapist`) until the lexicon supplies the correct composite.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad80b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = {\"我\", \"喜欢\", \"学习\", \"人工智能\", \"自然语言\", \"处理\"}\n",
    "\n",
    "def max_match(s, lex):\n",
    "    out, i = [], 0\n",
    "    while i < len(s):\n",
    "        for j in range(len(s), i, -1):\n",
    "            if s[i:j] in lex:\n",
    "                out.append(s[i:j])\n",
    "                i = j\n",
    "                break\n",
    "        else:\n",
    "            out.append(s[i])\n",
    "            i += 1\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9910501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: ['我', '喜欢', '学习', '人工智能']\n",
      "Jieba : ['我', '喜欢', '学习', '人工智能']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "sent = \"我喜欢学习人工智能\"\n",
    "print(\"Greedy:\", max_match(sent, LEX))\n",
    "print(\"Jieba :\", list(jieba.cut(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b0d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
